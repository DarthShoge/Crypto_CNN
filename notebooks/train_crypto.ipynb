{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False\n",
    "use_ramdon_split = False\n",
    "use_dataparallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "if use_gpu:\n",
    "    from utils.gpu_tools import *\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join([ str(obj) for obj in select_gpu(query_gpu())])\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "IMAGE_WIDTH = {5: 15, 16:64, 20: 60, 60: 180}\n",
    "IMAGE_HEIGHT = {5: 32, 16:48, 20: 64, 60: 96}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data\n",
    "\n",
    "here we choose 1993-2001 data as our training(include validation) data, the remaining will be used in testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156043, 48, 64)\n",
      "(156043, 10)\n"
     ]
    }
   ],
   "source": [
    "raw_images = []\n",
    "raw_label_df = []\n",
    "raw_images = pickle.load(open('../crypto/data/240_16_8_images.dat', 'rb'))\n",
    "raw_label_df = pd.read_feather('../crypto/data/240_16_8_labels.feather')\n",
    "raw_images = np.array(raw_images).reshape((-1, IMAGE_HEIGHT[16], IMAGE_WIDTH[16]))\n",
    "\n",
    "\n",
    "\n",
    "print(raw_images.shape)\n",
    "print(raw_label_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101427, 48, 64)\n",
      "(101427, 10)\n",
      "      Asset          Start_Date        Predict_Date            End_Date  \\\n",
      "29236   BTC 2013-10-17 09:00:00 2013-10-18 01:00:00 2013-10-19 01:00:00   \n",
      "29237   BTC 2013-10-18 09:00:00 2013-10-19 01:00:00 2013-10-20 01:00:00   \n",
      "29238   BTC 2013-10-19 09:00:00 2013-10-20 01:00:00 2013-10-21 01:00:00   \n",
      "29239   BTC 2013-10-20 09:00:00 2013-10-21 01:00:00 2013-10-22 01:00:00   \n",
      "29240   BTC 2013-10-21 09:00:00 2013-10-22 01:00:00 2013-10-23 01:00:00   \n",
      "\n",
      "      Daily_Return   Ret_24H    Ret_8H  Log_Ret_24H  Log_Ret_8H    Market_Cap  \n",
      "29236         None  0.036533  0.000000     0.000000    0.000000  1.795176e+09  \n",
      "29237         None  0.117262  0.105344     0.100157    0.100157  2.023532e+09  \n",
      "29238         None -0.021289  0.000000     0.000000    0.000000  2.042262e+09  \n",
      "29239         None  0.081870  0.043066     0.042165    0.042165  2.154779e+09  \n",
      "29240         None -0.149295  0.032019     0.031517    0.031517  2.298994e+09  \n",
      "       Asset          Start_Date        Predict_Date            End_Date  \\\n",
      "59589    FXS 2022-12-19 09:00:00 2022-12-20 01:00:00 2022-12-21 01:00:00   \n",
      "22166    BAT 2022-12-19 09:00:00 2022-12-20 01:00:00 2022-12-21 01:00:00   \n",
      "154902   ZEC 2022-12-19 09:00:00 2022-12-20 01:00:00 2022-12-21 01:00:00   \n",
      "56444    FIL 2022-12-19 09:00:00 2022-12-20 01:00:00 2022-12-21 01:00:00   \n",
      "135727  TEER 2022-12-19 09:00:00 2022-12-20 01:00:00 2022-12-21 01:00:00   \n",
      "\n",
      "       Daily_Return   Ret_24H    Ret_8H  Log_Ret_24H  Log_Ret_8H    Market_Cap  \n",
      "59589          None -0.002183  0.000000     0.000000    0.000000  3.628082e+08  \n",
      "22166          None  0.042232  0.038731     0.038000    0.038000  2.695548e+08  \n",
      "154902         None -0.010436 -0.000712    -0.000712   -0.000712  5.521068e+08  \n",
      "56444          None  0.015862  0.032414     0.031900    0.031900  9.681432e+08  \n",
      "135727         None  0.075269  0.000000     0.000000    0.000000  3.495144e+05  \n"
     ]
    }
   ],
   "source": [
    "# Calculate the index for 65% of the data\n",
    "split_index = int(0.65 * len(raw_images))\n",
    "\n",
    "# Split the images\n",
    "images = raw_images[:split_index]\n",
    "label_df = raw_label_df.iloc[:split_index]\n",
    "# Split the labels\n",
    "print(images.shape)\n",
    "print(label_df.shape)\n",
    "print(label_df.head())\n",
    "print(label_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img, label):\n",
    "        self.img = torch.Tensor(img.copy())\n",
    "        self.label = torch.Tensor(label)\n",
    "        self.len = len(img)\n",
    "\n",
    "        # Ensure the image tensor is in the shape [batch, height, width]\n",
    "        if len(self.img.shape) == 3:\n",
    "            # Add a channel dimension: [batch, 1, height, width]\n",
    "            self.img = self.img.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.img[idx], self.label[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split method (not random split is recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_ramdon_split:\n",
    "    train_val_ratio = 0.7\n",
    "    split_idx = int(images.shape[0] * 0.7)\n",
    "    train_dataset = MyDataset(images[:split_idx], (label_df.Ret_24H > 0).values[:split_idx])\n",
    "    val_dataset = MyDataset(images[split_idx:], (label_df.Ret_24H > 0).values[split_idx:])\n",
    "else:\n",
    "    dataset = MyDataset(images, (label_df.Ret_24H > 0).values)\n",
    "    train_val_ratio = 0.7\n",
    "    train_dataset, val_dataset = random_split(dataset, \\\n",
    "        [int(dataset.len*train_val_ratio), dataset.len-int(dataset.len*train_val_ratio)], \\\n",
    "        generator=torch.Generator().manual_seed(42))\n",
    "    del dataset\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 48, 64])\n"
     ]
    }
   ],
   "source": [
    "sample_input, _ = next(iter(train_dataloader))\n",
    "print(sample_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import baseline\n",
    "\n",
    "device = 'cuda' if use_gpu else 'cpu'\n",
    "export_onnx = True\n",
    "net = baseline.Net2(48,64).to(device)\n",
    "net.apply(init_weights)\n",
    "\n",
    "if export_onnx:\n",
    "    import torch.onnx\n",
    "    x = torch.randn([1,1,48,64]).to(device)\n",
    "    torch.onnx.export(net,               # model being run\n",
    "                      x,                         # model input (or a tuple for multiple inputs)\n",
    "                      \"../cnn_baseline.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                      export_params=False,        # store the trained parameter weights inside the model file\n",
    "                      opset_version=10,          # the ONNX version to export the model to\n",
    "                      do_constant_folding=False,  # whether to execute constant folding for optimization\n",
    "                      input_names = ['input_images'],   # the model's input names\n",
    "                      output_names = ['output_prob'], # the model's output names\n",
    "                      dynamic_axes={'input_images' : {0 : 'batch_size'},    # variable length axes\n",
    "                                     'output_prob' : {0 : 'batch_size'}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.0.weight : torch.Size([64, 1, 5, 3])\n",
      "layer1.0.bias : torch.Size([64])\n",
      "layer1.1.weight : torch.Size([64])\n",
      "layer1.1.bias : torch.Size([64])\n",
      "layer2.0.weight : torch.Size([128, 64, 5, 3])\n",
      "layer2.0.bias : torch.Size([128])\n",
      "layer2.1.weight : torch.Size([128])\n",
      "layer2.1.bias : torch.Size([128])\n",
      "layer3.0.weight : torch.Size([256, 128, 5, 3])\n",
      "layer3.0.bias : torch.Size([256])\n",
      "layer3.1.weight : torch.Size([256])\n",
      "layer3.1.bias : torch.Size([256])\n",
      "fc1.1.weight : torch.Size([2, 49152])\n",
      "fc1.1.bias : torch.Size([2])\n",
      "total_parameters : 715010\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for name, parameters in net.named_parameters():\n",
    "    print(name, ':', parameters.size())\n",
    "    count += parameters.numel()\n",
    "print('total_parameters : {}'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "FLOPs = 37.573623808G\n",
      "Params = 0.71501M\n"
     ]
    }
   ],
   "source": [
    "from thop import profile as thop_profile\n",
    "\n",
    "flops, params = thop_profile(net, inputs=(next(iter(train_dataloader))[0].to(device),))\n",
    "print('FLOPs = ' + str(flops/1000**3) + 'G')\n",
    "print('Params = ' + str(params/1000**2) + 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Code\\DigitalAssets\\liquid-vision\\venv\\lib\\site-packages\\torch\\autograd\\profiler.py:255: UserWarning: CUDA is not available, disabling CUDA profiling\n",
      "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  model_inference         1.14%       5.186ms       100.00%     454.540ms     454.540ms             1  \n",
      "                     aten::conv2d         0.06%     277.000us        64.85%     294.758ms      98.253ms             3  \n",
      "                aten::convolution         0.03%     140.000us        64.79%     294.481ms      98.160ms             3  \n",
      "               aten::_convolution         0.07%     322.000us        64.76%     294.341ms      98.114ms             3  \n",
      "         aten::mkldnn_convolution        64.34%     292.459ms        64.68%     294.019ms      98.006ms             3  \n",
      "                      aten::empty         0.41%       1.847ms         0.41%       1.847ms      57.719us            32  \n",
      "                aten::as_strided_         0.01%      38.000us         0.01%      38.000us      12.667us             3  \n",
      "                    aten::resize_         0.00%       8.000us         0.00%       8.000us       2.667us             3  \n",
      "                       aten::add_         0.02%      72.000us         0.02%      72.000us      24.000us             3  \n",
      "                 aten::batch_norm         0.08%     363.000us         9.13%      41.499ms      13.833ms             3  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 454.540ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "inputs = next(iter(train_dataloader))[0].to(device)\n",
    "\n",
    "with profile(activities=[\n",
    "        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        net(inputs)\n",
    "\n",
    "prof.export_chrome_trace(\"../trace.json\")\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, net, loss_fn, optimizer):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    current = 0\n",
    "    net.train()\n",
    "    \n",
    "    with tqdm(dataloader) as t:\n",
    "        for batch, (X, y) in enumerate(t):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = net(X)\n",
    "            loss = loss_fn(y_pred, y.long())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = (len(X) * loss.item() + running_loss * current) / (len(X) + current)\n",
    "            current += len(X)\n",
    "            t.set_postfix({'running_loss':running_loss})\n",
    "    \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(dataloader, net, loss_fn):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    current = 0\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(dataloader) as t:\n",
    "            for batch, (X, y) in enumerate(t):\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                y_pred = net(X)\n",
    "                loss = loss_fn(y_pred, y.long())\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                running_loss = (len(X) * running_loss + loss.item() * current) / (len(X) + current)\n",
    "                current += len(X)\n",
    "            \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = torch.load('/home/clidg/proj_2/pt/baseline_epoch_10_train_0.6865865240322523_eval_0.686580_.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu and use_dataparallel and 'DataParallel' not in str(type(net)):\n",
    "    net = net.to(device)\n",
    "    net = nn.DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-5)\n",
    "\n",
    "start_epoch = 0\n",
    "min_val_loss = 1e9\n",
    "last_min_ind = -1\n",
    "early_stopping_epoch = 5\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 555/555 [09:33<00:00,  1.03s/it, running_loss=1.07]\n",
      "100%|██████████| 119/119 [01:08<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 555/555 [08:21<00:00,  1.11it/s, running_loss=0.94] \n",
      "100%|██████████| 119/119 [01:07<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 555/555 [08:18<00:00,  1.11it/s, running_loss=0.878]\n",
      "100%|██████████| 119/119 [01:06<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 555/555 [07:48<00:00,  1.19it/s, running_loss=0.826]\n",
      "100%|██████████| 119/119 [01:05<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 555/555 [07:48<00:00,  1.18it/s, running_loss=0.797]\n",
      "100%|██████████| 119/119 [01:05<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 555/555 [07:45<00:00,  1.19it/s, running_loss=0.786]\n",
      "100%|██████████| 119/119 [01:05<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 555/555 [07:44<00:00,  1.20it/s, running_loss=0.768]\n",
      "100%|██████████| 119/119 [01:04<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 555/555 [07:43<00:00,  1.20it/s, running_loss=0.755]\n",
      "100%|██████████| 119/119 [01:04<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Best epoch: 2, val_loss: 0.6363375987745133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "os.mkdir('../pt'+os.sep+start_time)\n",
    "epochs = 50\n",
    "for t in range(start_epoch, epochs):\n",
    "    print(f\"Epoch {t}\\n-------------------------------\")\n",
    "    time.sleep(0.2)\n",
    "    train_loss = train_loop(train_dataloader, net, loss_fn, optimizer)\n",
    "    val_loss = val_loop(val_dataloader, net, loss_fn)\n",
    "    tb.add_histogram(\"train_loss\", train_loss, t)\n",
    "    torch.save(net, '../pt'+os.sep+start_time+os.sep+'baseline_epoch_{}_train_{:5f}_val_{:5f}.pt'.format(t, train_loss, val_loss)) \n",
    "    if val_loss < min_val_loss:\n",
    "        last_min_ind = t\n",
    "        min_val_loss = val_loss\n",
    "    elif t - last_min_ind >= early_stopping_epoch:\n",
    "        break\n",
    "\n",
    "print('Done!')\n",
    "print('Best epoch: {}, val_loss: {}'.format(last_min_ind, min_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
